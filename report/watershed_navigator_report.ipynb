{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Watershed Navigator: A RAG-Based AI Assistant for Environmental Analysis\n",
        "\n",
        "[Github Repository](https://github.com/zach-redder/watershed-navigator)\n",
        "\n",
        "## Introduction\n",
        "Watershed Navigator is a local Retrieval-Augmented Generation (RAG) based AI assistant built to answer environmental questions using relevant document contexts. Developed as an experimental integration for LimnoTech, the project explores practical ways AI could enhance LimnoTech's environmental solutions.\n",
        "\n",
        "**Real-World Goal:** To begin to experiment with how AI could be useful within a company such as LimnoTech, to get experience for my internship with them in the summer of 2025.\n",
        "\n",
        "**Technical Goal:** Understand how to locally host an LLM and learn the process of RAG.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Description\n",
        "\n",
        "The dataset consists of watershed-related PDF documents relevant to LimnoTech’s operations.\n",
        "\n",
        "\n",
        "- **Source & Types of Documents:**  \n",
        "  EPA reports, SEMCOG documents, internal papers, slide decks\n",
        "\n",
        "- **Number and Volume:**  \n",
        "  ~20 PDFs\n",
        "\n",
        "- **Rationale for Selection:**  \n",
        "  These documents related to the work that LimnoTech does and may be useful to clients' concerns\n",
        "\n",
        "### Dataset Strengths and Limitations\n",
        "- **Strengths:**  \n",
        "  Authoritative sources, specifially related to LimnoTech\n",
        "\n",
        "- **Limitations:**  \n",
        "  Narrow dataset, not highly researched by me (time constraints)\n",
        "\n",
        "--- "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Technical Approach\n",
        "\n",
        "Watershed Navigator employs a RAG approach combining semantic retrieval and generative AI.\n",
        "\n",
        "### Technologies Used\n",
        "- **UI**: Streamlit  \n",
        "- **Embeddings**: SentenceTransformers MiniLM  \n",
        "- **Retrieval**: Cosine similarity search  \n",
        "- **Generative Model**: TinyLLaMA (hosted locally via Ollama)\n",
        "\n",
        "### Technological Workflow\n",
        "- **Document Ingestion**: Loading, text chunking, embedding generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preload_documents(folder=\"data\"):\n",
        "    docs = []\n",
        "\n",
        "    for file in Path(folder).glob(\"*\"):\n",
        "        if file.suffix == \".pdf\":\n",
        "            text = extract_text_from_pdf(str(file))\n",
        "            chunks = chunk_text(text, chunk_size=1000)\n",
        "        elif file.suffix == \".csv\":\n",
        "            df = pd.read_csv(file)\n",
        "            chunks = [row.to_json() for _, row in df.iterrows()]\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        docs.extend([{\"text\": chunk, \"source\": str(file)} for chunk in chunks])\n",
        "\n",
        "    build_index(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Retrieval**: Query embedding, cosine similarity test, thresholding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def embed_text(text: str) -> np.ndarray:\n",
        "    return model.encode([text])[0]\n",
        "\n",
        "def load_embeddings(path=\"store/embeddings.pkl\"):\n",
        "    with open(path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def search(query: str, k: int = 3, threshold: float = 0.5):\n",
        "    docs = load_embeddings()\n",
        "    query_vec = embed_text(query).reshape(1, -1)\n",
        "    doc_vecs = np.array([doc[\"embedding\"] for doc in docs])\n",
        "    scores = cosine_similarity(query_vec, doc_vecs)[0]\n",
        "    top_indices = scores.argsort()[::-1][:k]\n",
        "    \n",
        "    if scores[top_indices[0]] < threshold:\n",
        "        return []  # Consider it unrelated\n",
        "\n",
        "    return [docs[i] for i in top_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Answer Generation**: Prompt construction/engineering, local generation (TinyLLaMa)\n",
        "- **UI/UX**: Flow of interaction, irrelevant query handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modeling Setup, Validation & Improvement\n",
        "\n",
        "- **Inputs**: User-entered questions that relate to watersheds and environmental issues\n",
        "- **Outputs**: Contextually accurate, informative answers generated by TinyLLaMA based on in-context learning\n",
        "\n",
        "### Validation\n",
        "- **Evaluation Metrics**: Qualitative manual evaluation/inspection, similarity scores\n",
        "\n",
        "### Attempted Improvements\n",
        "- Embedding model adjustments, prompt engineering changes, chunk-size tuning\n",
        "\n",
        "### Outcomes of Adjustments\n",
        "- Better accuracy, improved contextual coherence, reduced irrelevant outputs and hallucinations\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative Approaches\n",
        "\n",
        "### Hugging Face API Integration\n",
        "\n",
        "- Integrated Hugging Face's inference API to leverage cloud-based generative models as an alternative to local hosting.\n",
        "- Achieved initially promising results with improved model capabilities, encountered API rate-limit restraints\n",
        "- Had to switch to locally hosted modal for easier use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask_llama(prompt: str, max_new_tokens=512) -> str:\n",
        "    response = requests.post(\n",
        "        \"http://localhost:11434/api/generate\",\n",
        "        json={\n",
        "            \"model\": \"tinyllama\",\n",
        "            \"prompt\": prompt,\n",
        "            \"stream\": True,\n",
        "            \"options\": {\"num_predict\": max_new_tokens}\n",
        "        },\n",
        "        stream=True\n",
        "    )\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        raise RuntimeError(f\"❌ API error: {response.status_code} - {response.text}\")\n",
        "\n",
        "    final_response = \"\"\n",
        "    for line in response.iter_lines():\n",
        "        if line:\n",
        "            part = line.decode('utf-8')\n",
        "            json_data = json.loads(part)\n",
        "            if 'response' in json_data:\n",
        "                final_response += json_data['response']\n",
        "    return final_response.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experimentation with Larger Models\n",
        "\n",
        "- Experimented with hosting larger LLMs locally, but ran out of disk storage very fast\n",
        "- Larger models produced marginally higher-quality responses but required significantly greater computational resources\n",
        "\n",
        "### Vector Database\n",
        "\n",
        "- Attemped to use ChromaDB to implement embedding retrieval because of it's ease of initial setup\n",
        "- Ended up being hard to integrate and get properly working\n",
        "- FAISS significantly improved retrieval speed, especially as the dataset grew, providing quicker responses and improved scalability.\n",
        "\n",
        "### Prompt Engineering\n",
        "\n",
        "- Tested different prompt formats and lengths to optimize generation\n",
        "- Short, concise prompts improved response coherence but occasionally lacked necessary details.\n",
        "- Extensive context-rich prompts improved accuracy for complex questions but sometimes confused the model\n",
        "- Had to find a balanced prompt in the end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_prompt(context: str, question: str) -> str:\n",
        "    return f\"\"\"You are Watershed Navigator, an expert AI assistant specialized in environmental science,\n",
        "    watershed management, and stormwater infrastructure. Your role is to provide precise, detailed,\n",
        "    and contextually grounded responses based on provided document context and when necessary,\n",
        "    your general knowledge.\n",
        "\n",
        "Document Context:\n",
        "{context}\n",
        "\n",
        "User's Question:\n",
        "{question}\n",
        "\n",
        "Your detailed, professional response:\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Future Direction\n",
        "\n",
        "\n",
        "\n",
        "### Expand Document Dataset\n",
        "- Add more documents to the dataset\n",
        "- Add more qualified information from LimnoTech themselves\n",
        "\n",
        "### Use Larger Model\n",
        "- Experiment with quantized versions of larger open-source models, and switch to calling an API over hosting the model locally\n",
        "\n",
        "### Add Evaluation Framework\n",
        "- Introduce a structured evaluation protocol using domain-specific benchmarks or user surveys to measure relevance, factuality, and clarity.\n",
        "\n",
        "### User Feedback\n",
        "- Allow end-users to rate answers or flag information as incorrect to improve the model's performance over time\n",
        "\n",
        "\n",
        "--- "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "To summarize, this AI assistant demonstrates a way to integrate AI into an environmental consulting business. By combining local document retrieval with lightweight generative models, the assistant is capable of answering specialized questions using context. It highlights the ability of RAG to support AI workflows.\n",
        "\n",
        "This project successfully laid the groundwork for future integration within LimnoTech, and helped me to understand some of this before heading into my internship. It will serve as a usable blueprint for AI-powered applications and integration within environmental consulting work.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
